{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db96038",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2dfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa84d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\miniconda3\\envs\\jupyter_env\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import argparse\n",
    "from fuzzywuzzy import process\n",
    "import os\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function argparser\n",
    "def argument_parser():\n",
    "    # Create ArgumentParser with the app description\n",
    "    parser = argparse.ArgumentParser(description = 'This app find the BiciMAD/BiciPARK station closest to a set of public\\\n",
    "    schools')\n",
    "    # Create message to help to the users\n",
    "    help_message = 'You have two options:\\\n",
    "    \\n(1) str=\"All\": to get the table for every \"Place of interest\" included in the dataset (or a set of them).\\\n",
    "    \\n(2) str=school_name: to get the table for a specific \"public school\" imputed by the user.'  \n",
    "    # Use '-p' as a flag to select opcion 1 or 2\n",
    "    parser.add_argument('-p', '--parameter', help=help_message, type=str)\n",
    "    # Obtain argument\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d657429",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb9dbd",
   "metadata": {},
   "source": [
    "#### Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 'transform_df'\n",
    "def transform_df(df):\n",
    "    # Function to transform and clean the dataframe import from csv.\n",
    "    \n",
    "    # Extract the column, delete '[' and ']'. Split the string using ',' and convert to float. Store this data in two\n",
    "    # columns: longitude and latitude. Add this two columns to the original dataframe\n",
    "    temp_df = df['geometry.coordinates'].str.strip('[]').str.split(',', expand=True).astype('float64')\n",
    "    temp_df.columns = ['longitude', 'latitude']\n",
    "    df= pd.concat([df,temp_df],axis=1)\n",
    "    \n",
    "    # Delete column 'geometry.coordinates' and 'Unnamed: 0' columns\n",
    "    df = df.drop(['Unnamed: 0', 'geometry.coordinates'], axis=1)\n",
    "    \n",
    "    # Change the name of 'geometry.type' column becase include '.' in the name, and it could be a potential error\n",
    "    df = df.rename(columns={'geometry.type':'geometry_type'})\n",
    "    \n",
    "    # In case the column names were e.g. 'stationId', extract each column name, if includes 'station', repleace that for \n",
    "    # ' '. And change the string to lowercase\n",
    "    columns = df.columns.tolist()\n",
    "    new_column_names = [column_name.replace('station', '').lower() for column_name in columns]\n",
    "    df.columns = new_column_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# FUNCTION 'extract_dict2df'\n",
    "def extract_dict2df(df):\n",
    "    # Function to extract the dictionaries that are included in the cells of some of the columns. Create a new column for each \n",
    "    # key and store in it the corresponding values. Finally delete the original columns where the dictionaries are located.\n",
    "    # With this function it is possible to extract the dictionaries that are in different columns, regardless of the number of \n",
    "    # columns that have dictionaries or the number of items in each of them.\n",
    "    \n",
    "    column_names = df.columns.values   # Store the column names in a list called 'column_names'\n",
    "\n",
    "    # In this loop, iterate over the columns of the DataFrame\n",
    "    for col_name in column_names:\n",
    "        # Check if the first cell type is a dictionary and, in this case, check if it includes more than 1 items\n",
    "        if isinstance(df.at[0, col_name], dict) and len(df.at[0, col_name])>1:\n",
    "            # Extract the keys from the first dictionary found. Use '.at' to get a single value from the DataFrame.\n",
    "            keys = list(df.at[0, col_name].keys())\n",
    " \n",
    "            # Iterar sobre las claves y agregar nuevas columnas al DataFrame\n",
    "            for key in keys:\n",
    "                new_col_name = f\"{col_name}_{key}\"  # Nombre de la nueva columna\n",
    "                df[new_col_name] = df[col_name].apply(lambda x: x.get(key))\n",
    "\n",
    "            # Delete the previous column with the dictionaries inside each cell\n",
    "            df = df.drop(columns=[col_name])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb5d0e",
   "metadata": {},
   "source": [
    "#### Geo-calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35518764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mercator(lat, long):\n",
    "    # transform latitude/longitude data in degrees to pseudo-mercator coordinates in metres\n",
    "    c = gpd.GeoSeries([Point(lat, long)], crs=4326)\n",
    "    c = c.to_crs(3857)\n",
    "    return c\n",
    "\n",
    "def distance_meters(lat_start, long_start, lat_finish, long_finish):\n",
    "    # return the distance in metres between to latitude/longitude pair points in degrees \n",
    "    # (e.g.: Start Point -> 40.4400607 / -3.6425358 End Point -> 40.4234825 / -3.6292625)\n",
    "    start = to_mercator(lat_start, long_start)\n",
    "    finish = to_mercator(lat_finish, long_finish)\n",
    "    return start.distance(finish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb38e470",
   "metadata": {},
   "source": [
    "#### API EMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b72c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_emt(BASE_URL):\n",
    "    \"\"\"Summary: function to do login in emt mobility web\n",
    "\n",
    "    Args:\n",
    "        BASE_URL (string): string with the base url of the emt web\n",
    "\n",
    "    Returns:\n",
    "        accessToken (string): key necessary to extract the updated bicimad data\n",
    "    \"\"\"\n",
    "    # Extract the user data necessary to login\n",
    "    config = dotenv_values('../.env')\n",
    "    email_user = config.get('CLIENT_ID')   # Extract CLIENT ID from .env file.\n",
    "    password = config['CLIENT_SECRET']   # Extract CLIENT SECRET from .env file.\n",
    "\n",
    "    # Built the endpoint and header, make the get operation and extract the token.\n",
    "    ENDPOINT_LOGIN = \"v1/mobilitylabs/user/login/\"   # Part of the web adress to login.\n",
    "    url_login = BASE_URL + ENDPOINT_LOGIN   # Build the endpoint to login.\n",
    "    headers_longin = {\"email\": email_user, \"password\": password}   # Create the headerns needed to include in the get operation.\n",
    "    kwargs = {\"url\": url_login, \"headers\": headers_longin, \"timeout\": 10}   # Create the arguments to do the get.\n",
    "    response_emt_login = requests.get(**kwargs)   # Operation get.\n",
    "    response_emt_login = response_emt_login.json()   # Transform the data to json.\n",
    "    print(response_emt_login['code'])\n",
    "    print('Token dentro de la funci√≥n: ', accessToken)\n",
    "    \n",
    "    # If the response code is '00' means that the login operation is correct.\n",
    "    if (response_emt_login['code'] == '00') or (response_emt_login['code'] == '01'):\n",
    "        accessToken = response_emt_login['data'][0]['accessToken']   # Extract the accessToken from the json.\n",
    "        os.putenv(\"ACCESS_TOKEN\", accessToken)   # Store the token in '.env' file\n",
    "        return accessToken\n",
    "    else:\n",
    "        print('Error in the comunication with the emt web')  \n",
    "\n",
    "\n",
    "def extract_bicimad_data_emt(BASE_URL, accessToken):\n",
    "    \"\"\"Summary: function to extract updated bicimad data\n",
    "\n",
    "    Args:\n",
    "        BASE_URL (string): url base of the emt web\n",
    "        accessToken (string): key necessary to extract the updated bicimad data\n",
    "\n",
    "    Returns:\n",
    "        bicimad_data (dictionary): bicimad data\n",
    "    \"\"\"\n",
    "    # Extract the token necessary to extract data.\n",
    "    config = dotenv_values('../.env')\n",
    "    accessToken = config.get('ACCESS_TOKEN')   # Extract TOKEN from .env file.\n",
    "\n",
    "    # Built the endpoint and header, make the get operation and extract the information related to bicimad station.\n",
    "    ENDPOINT_STATIONS = \"v1/transport/bicimad/stations/\"   # Part of the web adress to extract bicimad data.\n",
    "    url_stations = BASE_URL + ENDPOINT_STATIONS   # Build the endpoint to login.\n",
    "    headers = {\"accessToken\": accessToken}   # Create the headerns needed to include in the get operation.\n",
    "    kwargs = {\"url\": url_stations, \"headers\": headers, \"timeout\": 10}   # Create the arguments to do the get.\n",
    "    response_emt_station = requests.get(**kwargs)   # Operation get.\n",
    "    response_emt_station = response_emt_station.json()   # Transform the data to json.\n",
    "\n",
    "    return response_emt_station\n",
    "\n",
    "\n",
    "def process_json(json_data):\n",
    "    # The dictionary has two keys: '@context' and '@graph'. And the interesting data are in the value of the second key where\n",
    "    # other dictionaries are included. Extract both keys in a list called 'keys' -> json_data[\"@graph\"] = json_data[keys[1]].\n",
    "    keys=list(json_data.keys())\n",
    "    # Create the dataframe with the data stored in '@graph'. This way, if the name of the dictionary change, it will still work.\n",
    "    df = pd.DataFrame(json_data[keys[1]])\n",
    "    return df\n",
    "\n",
    "\n",
    "def import_update_json():\n",
    "    \"\"\"Summary: function to import the data from web. This funcion uses \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Extract the token necessary to extract data.\n",
    "    config = dotenv_values('.env')\n",
    "    accessToken = config.get('ACCESS_TOKEN')   # Extract TOKEN from .env file.\n",
    "    print('Token al principio del todo: ', accessToken)\n",
    "\n",
    "    BASE_URL = \"https://openapi.emtmadrid.es/\"   # Base url of the web.\n",
    "    json__response = extract_bicimad_data_emt(BASE_URL, accessToken)\n",
    "    \n",
    "    # Check if the access token is stil valid. If the token is expired, excute the login function again and create new acess token. With that, \n",
    "    # the login operation only is executed when the token is expired. \n",
    "    if (json__response['code'] != '00') or  (json__response['description'] == 'Error, token not found in cache'):\n",
    "        print('The token stored is expired. The program will be login again and create new access token')\n",
    "        accessToken = login_emt(BASE_URL)\n",
    "        print('Token despu√©s de generarlo de nuevo: ', accessToken)\n",
    "        # Execute the function to extract the updated bicimad data again with the new token\n",
    "        json__response_data =  extract_bicimad_data_emt(BASE_URL, accessToken)\n",
    "        print(json__response_data)\n",
    "    \n",
    "    json_data = json__response['data'][0]   # Extract the bicimad data from the json.\n",
    "    df = process_json(json_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca79ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token al principio del todo:  None\n",
      "The token stored is expired. The program will be login again and create new access token\n",
      "01\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'accessToken' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mimport_update_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 86\u001b[0m, in \u001b[0;36mimport_update_json\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (json__response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m  (json__response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError, token not found in cache\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe token stored is expired. The program will be login again and create new access token\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m     accessToken \u001b[38;5;241m=\u001b[39m \u001b[43mlogin_emt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken despu√©s de generarlo de nuevo: \u001b[39m\u001b[38;5;124m'\u001b[39m, accessToken)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# Execute the function to extract the updated bicimad data again with the new token\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mlogin_emt\u001b[1;34m(BASE_URL)\u001b[0m\n\u001b[0;32m     21\u001b[0m response_emt_login \u001b[38;5;241m=\u001b[39m response_emt_login\u001b[38;5;241m.\u001b[39mjson()   \u001b[38;5;66;03m# Transform the data to json.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_emt_login[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToken dentro de la funci√≥n: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43maccessToken\u001b[49m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# If the response code is '00' means that the login operation is correct.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (response_emt_login[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (response_emt_login[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'accessToken' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import_update_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c23e6",
   "metadata": {},
   "source": [
    "## Acquisition and wrangling: import, clean and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eca21c",
   "metadata": {},
   "source": [
    "#### Import and clean bicimap.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb13fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv and store the data in a dataframe\n",
    "bicimad_df = pd.read_csv(\"../data/raw/bicimad_stations.csv\", sep='\\t')\n",
    "# Clean and transform the dataframe\n",
    "bicimad_df = transform_df(bicimad_df)\n",
    "# Remove the number (e.g. '1a - , 1b - ....') from the name of each bicimap station\n",
    "bicimad_df['name'] = bicimad_df['name'].apply(lambda row: row.split(' - ')[1])\n",
    "bicimad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bicimad_df.to_csv(f\"../data/processed/bicimad.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1babde3",
   "metadata": {},
   "source": [
    "#### Import and clean bicipark.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffe1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv and store the data in a dataframe\n",
    "bicipark_df = pd.read_csv(\"../data/raw/bicipark_stations.csv\", sep=';')\n",
    "# Clean and transform the dataframe\n",
    "bicipark_df = transform_df(bicipark_df)\n",
    "# Remove the string 'bicipark ' from the name of each bicipark station\n",
    "bicipark_df['name'] = bicipark_df['name'].apply(lambda row: row.split('Bicipark ')[1])\n",
    "bicipark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ddad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bicipark_df.to_csv(f\"../data/processed/bicipark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddc8c3",
   "metadata": {},
   "source": [
    "#### Fix and prepare biciMAD and BiciPark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e17d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with columns that these will be use in the final app. \n",
    "# Extract some columns from bicimad.df in a new dataframe\n",
    "new_column_names = {'name': 'station_name', 'address': 'station_location', 'latitude': 'latitude', 'longitude': 'longitude'}\n",
    "bicimad_stations_df = bicimad_df[list(new_column_names.keys())].rename(columns=new_column_names)\n",
    "bicimad_stations_df['station_type'] = 'BiciMAD'\n",
    "bicimad_stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some columns from bicipark.df in a new dataframe\n",
    "new_column_names = {'name': 'station_name', 'address': 'station_location', 'latitude': 'latitude', 'longitude': 'longitude'}\n",
    "bicipark_stations_df = bicipark_df[list(new_column_names.keys())].rename(columns=new_column_names)\n",
    "bicipark_stations_df['station_type'] = 'BiciPARK'\n",
    "bicipark_stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09109c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pd.concat([bicimad_stations_df, bicipark_stations_df])\n",
    "stations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1b8a8",
   "metadata": {},
   "source": [
    "#### Import and clean json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c79246",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://datos.madrid.es/egob/catalogo/202311-0-colegios-publicos.json'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain json data\n",
    "json_data = response.json()\n",
    "\n",
    "# The dictionary has two keys: '@context' and '@graph'. And the interesting data are in the value of the second key where\n",
    "# other dictionaries are included. Extract both keys in a list called 'keys' -> json_data[\"@graph\"] = json_data[keys[1]]\n",
    "keys=list(json_data.keys())\n",
    "\n",
    "# Create the dataframe with the data stored in '@graph'. This way, if the name of the dictionary change, it will still work.\n",
    "public_schools_df = pd.DataFrame(json_data[keys[1]])\n",
    "\n",
    "# Use the 'extract_dict2df' function to extract the diccionaries included in some columns and create new columns with them.\n",
    "public_schools_df = extract_dict2df(public_schools_df) \n",
    "public_schools_df['organization_organization-desc'][3]   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some columns from bicipark.df in a new dataframe\n",
    "new_column_names = {'title': 'school_name', 'address_street-address': 'school_location', \n",
    "                    'location_latitude': 'latitude', 'location_longitude': 'longitude'}\n",
    "schools_df = public_schools_df[list(new_column_names.keys())].rename(columns=new_column_names)\n",
    "schools_df['place_type'] = 'Colegios p√∫blicos'\n",
    "schools_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9bcdc",
   "metadata": {},
   "source": [
    "## Analysis: calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7da41",
   "metadata": {},
   "source": [
    "#### Geo-calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use a dataframe fragment to test the code\n",
    "schools_test = schools_df#.iloc[0:2]\n",
    "bicimad_test = stations_df#.iloc[0:10]\n",
    "# Filter the dataframe and extract only the rows related to bicimad\n",
    "#bicimad_test = stations_df.loc[stations_df['station_type'] == 'BiciMAD'].iloc[0:10]\n",
    "\n",
    "# Merge public schools and bicimad/bicipark dataframe. Before the merge, create a new column called 'key' which value will\n",
    "# be '1'. Merge using this column and use drop to remove this column in the merged dataframe. This new dataframe will be \n",
    "# a dataframe in which \n",
    "merge_df = pd.merge(schools_test.assign(key=1), bicimad_test.assign(key=1), on='key').drop('key', axis=1)\n",
    "# Obtain the distance\n",
    "merge_df['distance'] = merge_df.apply(lambda row: distance_meters(row['latitude_x'], row['longitude_x'],\n",
    "                                                             row['latitude_y'], row['longitude_y']), axis=1)\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7050a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dataframe in '.csv' and this way I won't have to wait for the distance calculations to be performed if I want \n",
    "# to work with the dataframe\n",
    "merge_df.to_csv(\"../data/processed/distance_calculated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b40eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to work with them\n",
    "merge_df = pd.read_csv(\"../data/processed/distance_calculated.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short the dataframe for each school from minimum to maximum of the distance from each station to the school that \n",
    "# corresponds to it. Reset index and remove the new column index created\n",
    "merge_short_df = merge_df.sort_values(by=['school_name', 'distance']).reset_index().drop('index', axis=1)\n",
    "# To extract only the biciMAD items, it's neccesary to apply a filter\n",
    "bicimad_filter = merge_short_df['station_type'] == 'BiciMAD'\n",
    "# Obtain the resulting dataframe with the school and bicimad station with minimum distance. As the dataframe is already \n",
    "# sorted, with the distance values from smallest to largest, only the first value for each school needs to be extracted. \n",
    "# To do this, it's neccesary to apply the filter calculated above.\n",
    "minimum_df = merge_short_df[bicimad_filter].groupby('school_name').head(1)\n",
    "minimum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcde796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names and select the desired columns to adapt the result to the objective\n",
    "# Create a dictionary with the old and new column names\n",
    "new_columns_names = {'school_name': 'Place of interest',\n",
    "                     'place_type': 'Type of place',\n",
    "                     'school_location': 'Place address',\n",
    "                     'station_name': 'BiciMAD station',\n",
    "                     'station_location': 'Station location'}\n",
    "# Extract the interested columns and rename them.\n",
    "result_df = minimum_df[list(new_columns_names.keys())].rename(columns=new_columns_names).reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5065d60",
   "metadata": {},
   "source": [
    "## Store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34339224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a new '.csv' file\n",
    "result_df.to_csv(\"../data/result/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca71b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_school(df, station_type, school_name):\n",
    "    # Short dataframe and extract the columns interesting to the goal\n",
    "    #df = short_store_data(df, station_type)\n",
    "\n",
    "    # Create a filter with the rows that includes the specific lab\n",
    "    filter_df = df['Place of interest'] == school_name\n",
    "    # Evaluate if at least one element in condition is True. If True, it means that there is at least one row that meets the condition. If not\n",
    "    # the return is a error message\n",
    "    if filter_df.any():\n",
    "        return df[filter_df]\n",
    "    else:\n",
    "        return 'Error: the name of the lab you typed was not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3745448",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = show_one_school(result_df, 'bicimad', 'Colegio P√∫blico Adolfo Su√°rez')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c003f14",
   "metadata": {},
   "source": [
    "## Bonus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bonus was done in '.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451387b",
   "metadata": {},
   "source": [
    "## Bonus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced28f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_bicimad(df, name_school):\n",
    "    # With this function it is possible to get the best match for 'school_name' in the different school names\n",
    "    best_match = process.extractOne(name_school, result_df['Place of interest'])\n",
    "    best_match = process.extractOne(school_name, df['Place of interest'])\n",
    "    print(best_match)\n",
    "\n",
    "    # If the coincidence is higher than 80%, show the BiciMAD station\n",
    "    if best_match[1] >= 80:  \n",
    "        # Extract the row of the chosen school\n",
    "        choice_school = df.loc[result_df['Place of interest'] == best_match[0]]\n",
    "        # Extract the value bicimad station name\n",
    "        bicimad_nearest = choice_school['BiciMAD station'].values\n",
    "        # Show the result\n",
    "        return f\"The nearest BiciMAD station to the school {name_school} is {bicimad_nearest}.\"\n",
    "    else:\n",
    "        return \"No close match was found for the school name.\"\n",
    "    \n",
    "result_str = find_nearest_bicimad(result_df, 'adolfo suarez')\n",
    "result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_df\n",
    "best_match = process.extractOne('adolfo suarez', df['Place of interest'])\n",
    "if best_match[1] >= 80:\n",
    "    filter_df = df['Place of interest'] == best_match[0]\n",
    "    result_df = df[filter_df]\n",
    "\n",
    "result_all = result_df\n",
    "result_df['BiciMAD station'][0]\n",
    "\n",
    "filter_station = bicimad_df['name'] == result_df['BiciMAD station'][0]\n",
    "result_station_df =bicimad_df[filter_station]\n",
    "result_station_df.head()\n",
    "\n",
    "# Create list with only the interesting columns\n",
    "interesting_columns = ['total_bases', 'dock_bikes', 'free_bases']\n",
    "# Create list with the new names to rename the columns\n",
    "new_columns_names = ['Total bases', 'Dock bikes', 'Free bases']\n",
    "\n",
    "# Extract the free bases from the dataframe and include that with result_df\n",
    "data_to_insert = list(result_station_df[interesting_columns].values)\n",
    "print(data_to_insert)\n",
    "# Store this data in the dataframe with the other information\n",
    "result_all[interesting_columns] = data_to_insert\n",
    "\n",
    "result_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53865230",
   "metadata": {},
   "source": [
    "## Bonus 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = import_update_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87556633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pm1_env)",
   "language": "python",
   "name": "pm1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
